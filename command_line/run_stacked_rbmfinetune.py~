import tensorflow as tf
import numpy as np

import stacker_rbm_finetune
import datasets

# #################### #
#   Flags definition   #
# #################### #
flags = tf.app.flags
FLAGS = flags.FLAGS

# Global configuration
flags.DEFINE_string('dataset', 'mnist', 'Which dataset to use. ["mnist"]')
flags.DEFINE_string('model_name', 'srbm', 'Name of the model.')
flags.DEFINE_boolean('encode_train', False, 'Whether to encode and store the training set.')
flags.DEFINE_boolean('encode_valid', False, 'Whether to encode and store the validation set.')

# Supervised fine tuning parameters
flags.DEFINE_string('params_path', 'path1,path2,path3', 'Path to params .npz file.')
flags.DEFINE_string('layers', '256,256,256', 'Layers of the stack.')
flags.DEFINE_float('learning_rate', 0.01, 'Learning rate.')
flags.DEFINE_integer('n_iter', 10, 'Number of epochs.')
flags.DEFINE_integer('batch_size', 10, 'Size of each mini-batch.')
flags.DEFINE_string('opt', 'gradient_descent', 'Which optimizer to use.')
flags.DEFINE_string('loss_func', 'mean_squared', 'Loss function.')
flags.DEFINE_float('dropout', 1, 'Dropout parameter.')
flags.DEFINE_integer('verbose', 0, 'Level of verbosity. 0 - silent, 1 - print accuracy.')
flags.DEFINE_string('directory_name', 'srbm/', 'Directory to store data relative to the algorithm.')


# Parameters validation
assert FLAGS.dataset in ['mnist']

if __name__ == '__main__':

    if FLAGS.dataset == 'mnist':

        # ################# #
        #   MNIST Dataset   #
        # ################# #

        trX, trY, vlX, vlY, teX, teY = datasets.load_mnist_dataset()

        weights = []
        hbiases = []
        vbiases = []

        paths = [_ for _ in FLAGS.params_path.split(',') if _]
        layers = [int(_) for _ in FLAGS.layers.split(',') if _]

        # Load RBMs parameters
        for p in paths:

            rbm_params = np.load(p)['arr_0'].tolist()

            weights.append(rbm_params['W'])
            hbiases.append(rbm_params['bh_'])
            vbiases.append(rbm_params['bv_'])

        # Create the object
        srbm = stacker_rbm_finetune.StackedRBM(
            weights, hbiases, vbiases, layers,
            dataset=FLAGS.dataset, learning_rate=FLAGS.learning_rate, n_iter=FLAGS.n_iter,
            batch_size=FLAGS.batch_size, opt=FLAGS.opt, loss_func=FLAGS.loss_func, dropout=FLAGS.dropout,
            verbose=FLAGS.verbose, model_name=FLAGS.model_name, directory_name=FLAGS.directory_name)

        # Fit the model (unsupervised pretraining)
        srbm.finetune(trX, vlX)

        encoded_X = srbm.transform(trX, 'train', FLAGS.encode_train)
        encoded_vX = srbm.transform(vlX, 'validation', FLAGS.encode_valid)

        np.save('data/srbm/encodedX', encoded_X)
        np.save('data/srbm/encodedvX', encoded_vX)
